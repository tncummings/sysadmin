:doctype: book

Chapter 28 - Virtualization Overview

= Chapter 28 - Virtualization Overview

== Chapter 28 Introduction

Virtualization entails creating (in software) an abstracted complete machine environment, under which end user software runs unaware of the physical details of the machine it is is running on.

While there are quite a few approaches to accomplishing this mission, they generally involve a host operating system and filesystem which directly use the hardware.
One or more guest systems run on top (or underneath, depending how you look at it) of the host system at a lower level of privilege.
Access to hardware such as network adapters is usually done through some kind of virtual device which may access the physical device when required.

There are many uses for virtual machines.
Major ones include more efficient use of hardware, software isolation, security, stability, safe development options, and the ability to run multiple operating systems at the same time without rebooting.

== Learning Objectives

By the end of this chapter, you should be able to:

* Understand the concepts behind virtualization and how it came to be widely used.
* Understand the roles of hosts and guests.
* Discuss the difference between emulation and virtualization.
* Distinguish between the different types of hypervisors.
* Be familiar with how Linux distributions use and depend on libvirt.
* Use the qemu hypervisor.
* Install, use and manage KVM.

== What Is Virtualization?

In this chapter, we will be concerned with the creation, deployment, and maintenance of Virtual Machines (VMs).

Virtual Machines are a virtualized instance of an entire operating system, and may fulfill the role of a server or a desktop/workstation.

The outside world sees the VM as if it were an actual physical machine, present somewhere on the network.
Applications running in the VM are generally unaware of their non-physical environment.

=== Other Kinds of Virtualization

*_Network_*: The details of the actual physical network, such as the type of hardware, routers, etc.
are abstracted and need not be known by software running on it and configuring it.
Examples: Software Defined Networking or SDN.

*_Storage_*: Multiple network storage devices are configured to look like one big storage unit, such as a disk.
Examples: Network Attached Storage or NAS.

*_Application_*: An application is isolated into a standalone format, such as a container, which will be discussed later.

There are differences between physical and virtual machines which are not always easy to ignore.
For example, performance tuning at a low level needs to be done (separately) on both the VM and the physical machine residing underneath it.

== Virtualization History

Virtualization has a long proud history.
It was implemented originally on mainframes decades ago:

* Enables better hardware utilization
* Operating systems often progress more quickly than hardware
* It is microcode-driven
* Not particularly user-friendly.

Later on, virtualization technology migrated to PCs and workstations:

* Initially, it was done using emulation
* CPUs enhanced to support virtualization led to a boost in performance, easier configuration, and more flexibility in VM installation and migration.

From early mainframes to mini-computers, virtualization has been used for expanding limits, debugging and administration enhancements.

Today, virtualization can be found everywhere in many forms.
Each of the different forms and implementations that are available provide particular advantages.

== Hosts and Guests

A host is the underlying physical operating system managing one or more virtual machines.

A guest is the VM which is an instance of a complete operating system, running one or more applications.
Sometimes, a guest is also called a client.
In most cases, the guest should not care what host it is running on and can be migrated from one host to another, sometimes while actually running.

In fact, it is possible to convert physical machines into virtual ones, by copying the entire contents into a VM.
Specialized software utilities can make this easier.

Low-level performance tuning on areas such as CPU utilization, networking throughput, memory utilization, is often best done on the host, as the guest may have only simulated qualities not directly useful.

Application tuning will be done mostly on the guest.

== Emulation vs. Virtualization

The first implementations of virtualization on the PC architecture were through the use of emulators.
An application running on the current operating system would appear to another OS as a specific hardware environment.
Emulators generally do not require special hardware to operate.

Qemu is one such emulator.
image:../../_resources/129b4eb46a334ac48b1058d439e8392e.png[acadd23f96250bb37a19afb9fe0c1ae5.png]

=== Emulator

An Emulator runs completely in software.
Hardware constructs are replaced by software.
It is useful for running virtual machines on different architectures, such as running a pretend ARM guest machine on an X86 host.
Emulation is often used for developing an operating system for a new CPU, even before hardware is available.
Performance is relatively slow.

== Types of Virtualization Hypervisors

The host system, besides functioning normally with respect to software that it runs, also acts as the hypervisor that initiates, terminates, and manages guests.
It also called Virtual Machine Monitor (VMM).

There are two basic methods of virtualization:

. *_Hardware Virtualization_*: The guest system runs without being aware it is running as a virtualized guest, and does not require modifications to be run in this fashion.
It is also known as *Full Virtualization*.
. *_Para-virtualization_*: The guest system is aware it is running in a virtualized environment, and has been modified specifically to work with it.

Widely-used CPUs from Intel and AMD incorporate virtualization extensions to the x86 architecture that allow the hypervisor to run fully virtualized (i.e., unmodified) guest operating systems with only minor performance penalties.

The Intel extension (Intel Virtualization Technology), usually abbreviated as VT, IVT, VT-32 or VT-64, is also known under the development code name of Vanderpool.
It has been available since the spring of 2005.

The AMD extension is usually called AMD-V and is still sometimes referred to by the development code name of Pacifica.

For a detailed explanation and comparison of how these two extensions work, see the https://lwn.net/Articles/182080/[Xen and the new processors article].

You can check directly if your CPU supports hardware virtualization extensions by looking at /proc/cpuinfo;
if you have an IVT-capable chip, you will see vmx in the flags field;
and, if you have an AMD-V capable chip, you will see svm in the same field.
You may also have to ensure the virtualization capability is turned on in your CMOS.

While the choice of operating systems tends to be more limited for para-virtualized guests, originally they tended to run more efficiently than fully virtualized guests.
Advances in virtualization techniques have narrowed or eliminated such advantages, and the wider availability of the hardware support needed for full virtualization has made para-virtualization less advantageous and less popular.

Most modern hardware has hardware virtualization abilities (must be turned on in BIOS).

The hypervisor can be:

* External to the host operating system kernel: VMware
* Internal to the host operating system kernel: KVM.

In this course, we will concentrate on KVM, as it is all open source, and requires no external third party hypervisor program.

== Dedicated Hypervisor

Going past Emulation, the merging of the hypervisor program into a specially-designed lightweight kernel was the next step in the Virtualization deployment.

VMware ESX (and related friends) is an example of a hypervisor embedded into an operating system.
image:../../_resources/f671a3b8166f486d8f2b182472c408eb.png[1ae2f29626b551874374c2c1a66bb228.png]

== Hypervisor in the Kernel

The KVM project added hypervisor capabilities into the Linux kernel.
This leveraged the facilities of the kernel and enabled the kernel to be a hypervisor.

As we have discussed, specific CPU chip functions and facilities were required and deployed for this type of virtualization.
image:../../_resources/33c13b3811aa437fb7a60b9b317d4577.png[c28246aaf112b1a3ec12895a1295a9ed.png]

== libvirt

The libvirt project is a toolkit to interact with virtualization technologies.
It provides management for virtual machines, virtual networks, and storage, and is available on all enterprise Linux distributions.

Many application programs interface with libvirt, and some of the most common ones are virt-manager, virt-viewer, virt-install, virsh.

The complete list of currently supported hypervisors can be found on the https://www.libvirt.org/[libvirt web site]:

* QEMU/KVM
* Xen
* Oracle VirtualBox
* VMware ESX
* VMware Workstation/Player
* Microsoft Hyper-V
* IBM PowerVM (phyp)
* OpenVZ
* UML (User Mode Linux)
* LXC (Linux Containers)
* Virtuozzo
* Bhyve (The BSD Hypervisor)
* Test (Used for testing).

== Programs Using libvirt

There are many utilities using libvirt.
The exact list will depend on your Linux distribution.
A full list can be found on the https://www.libvirt.org/apps.html[libvirt website].

In this course, we will work through the use of the robust GUI, virt-manager, rather than make much use of command line utilities, which lead to more flexibility, as well as use on non-graphical servers.
image:../../_resources/664bd17fcfe248759c2b311c1bbc460c.png[9939eb356b1f9588f9c8c27b23602363.png]

== What Is QEMU?

QEMU stands for Quick EMUlator.
It was originally written by Fabrice Bellard in 2002.
(Bellard is also known for feats such as holding, at one point, the world record for calculating Ï€, reaching 2.7 trillion digits.)

QEMU is a hypervisor that performs hardware emulation, or virtualization.
It emulates CPUs by dynamically translating binary instructions between the host architecture and the emulated one.

The host and the emulated architectures may be different, or the same.
There are numerous choices for both host and guest operating systems.

QEMU can also be used to emulate just particular applications, not entire operating systems.

By itself, QEMU is much slower than the host machine.
But, it can be used together with KVM (Kernel Virtual Machine) to reach speeds close to those of the native host.

Guest operating systems do not require rewriting to run under QEMU.
QEMU can save, pause, and restore a virtual machine at any time.
QEMU is a free software licensed under the GPL.

QEMU has the capability of supporting many architectures, including: IA-32 (i386), x86-64, MIPS, SPARC, ARM, SH4, PowerPC, CRIS, MicroBlaze, etc.

The cross-compilation abilities of QEMU make it extremely useful when doing development for embedded processors.

In fact, QEMU has often been used to develop processors which have either not yet been physically produced, or released to market.

== Third Party Hypervisor Integration

Used by itself, QEMU is relatively slow.
However, it can be integrated with third party hypervisors, and then reach near native speeds.
Note that some of these systems are very close cousins of QEMU;
others are more remote.
To mention a few main ones:

* KVM offers particularly tight integration with QEMU.
When the host and the target are the same architecture, full acceleration and high speed are delivered.
KVM is native to Linux.
We will discuss this in detail.
* Xen, also native to Linux, can run in hardware virtualization mode if the architecture offers it, as does x86 and some ARM variants.
* Oracle Virtual Box can use qcow2 formatted images, and has a very close relationship with QEMU.

In this course, we recommend using (and will use in labs) `virt-manager` to configure and run virtual machines.
We will also give instructions on how to run them using `qemu` command line utilities.

== Image Formats

QEMU supports many formats for disk image files.
However, only two are used primarily, with the rest being available for historical reasons and for conversion utilities.

=== Disk Image Formats

*_raw_*: This is the default image format.
It is the simplest and easiest format to export to other non-QEMU emulators.
Empty sectors do not take up space.

*_qcow2_*: COW stands for Copy On Write.
There are many options.
See man qemu-img for more details.

To get a list of supported formats:

` c7:/tmp> qemu-img --help | grep formats: Supported formats: vfat vpc vmdk vhdx vdi ssh sheepdog rbd raw host_cdrom host_floppy host_device file \qed qcow2 qcow parallels nbd iscsi gluster dmg tftp ftps ftp https http cloop bochs blkverify blkdebug `

Note in particular:

* vdi: Used by Oracle Virtual Box
* vmdk: Used by VMware.

Note that `qemu-img` can be used to translate between formats, .e.g.:

`$ qemu-img convert -O vmdk myvm.qcow2 myvm.vmdk`

using default options.
See the man page for full possibilities.

== KVM and Linux

KVM uses the Linux kernel for computing resources, including memory management, scheduling, synchronization, and more.
When running a virtual machine, KVM engages in a co-processing relationship with the Linux kernel.

In this format, KVM runs the virtual machine monitor within one or more of the CPUs, using VMX or SVM instructions.
At the same time, the Linux kernel is executing on the other CPUs.

The virtual machine monitor runs the guest, which is running at full hardware speed, until it executes an instruction that causes the virtual machine monitor to take over.

At this point, the virtual machine monitor can use any Linux resource to emulate a guest instruction, restart the guest at the last instruction, or do something else.

By loading the KVM modules and starting a guest, you turn Linux into a hypervisor.
The Linux personality is still there, but you also have the hardware virtual machine monitor.
You can control the Virtual Machine using standard Linux resource and process control tools, such as cgroups, nice, numactl, and so on.

KVM first appeared (pre-merge) as part of a Windows Virtual Desktop product.
At the time of its upstream merge in 2007, KVM required recent x86_64 processors.
On x86_64 platforms, KVM is primarily (but not always) a driver for the processor's virtualization subsystem.

KVM appeared as a trio of Linux kernel modules in 2007.
When paired with a modified version of QEMU (also provided at the same time), it created a hypervisor that used the Linux kernel for most of its runtime services.

Shortly after Avi Kivity, the author of KVM, submitted the source code to the Linux development community, Linus merged KVM into his Linux tree.
This was surprising to a lot of folks.

== Managing KVM

There are many low level commands for creating, converting, manipulating, deploying, and maintaining virtual machine images.

Managing KVM can be done with both command line and graphical interfaces.

Command line tools include: virt-* and qemu-*.
Graphical interfaces include virt-manager, kimchi, OpenStack, oVirt, etc.

As you develop more expertise, you will become practiced in using them.
But, for all basic operations, virt-manager will suffice and that is what we will use.
image:../../_resources/37a6eb2284f14d279d50d4e16ba55899.png[7b3929283554aa49f7616fd668dfe6e9.png]

== Lab 28.1: Making Sure KVM is Properly Set up

. First check that you have hardware virtualization available and enabled: `$ grep -e vmx -e svm /proc/cpuinfo` where vmx is for INTEL CPUs and svm for AMD.
If you do not see either one of these: If  you  are  on  a  physical  machine,  maybe  you  can  fix  this.
Reboot  your  machine  and  see  if  you  can  turn  on virtualization in the BIOS settings.
Note that some IT personnel may make this impossible for "`security`" reasons, so try to get that policy changed.
You are on a virtual machine running under a hypervisor, and you do not have nested virtualization operable.
. If for either of these reasons, you do not have hardware virtualization, you may be able to run `virt-manager`, but with weak performance.
. You need all relevant packages installed on your system.
One can work hard to construct an exact list.
However, exact names and requirements change with time, and most enterprise distributions ship with all (or almost all) of the software you need.
. The easiest and best procedure is to run the script we have already supplied to you: `$ ./ready-for.sh --install` LFS201 where we have done the hard work.
Alternatively, on RPM systems you can do some overkill with: `$ sudo  yum|dnf|zypper install kvm* qemu* libvirt*` It is not a large amount of storage space to do it this way.
On Debian package based systems including Ubuntu you will have to do the equivalent with your favorite package installing procedure.

* Do not run libvirtdat the same time as another hypervisor as dire consequences are likely to arise.
This can easily include crashing your system and doing damage to any virtual machines being used.
* We recommend both stopping and disabling your other hypervisor as in: `$ sudo systemctl stop vmware` `$ sudo systemctl disable vmware` or `$ sudo systemctl stop vboxdrv` `$ sudo systemctl disable vboxdrv`

== Lab 28.2: Using virt-manager with KVM to Install a Virtual Machine and Run it

In  this  exercise  we  will  use  pre-built iso images  built  by https://www.tinycorelinux.net[TinyCoreLinux]  because  they  are cooked up very nicely and are quite small.
If  you  would  like,  you  can  substitute  any  installation iso image  for  another Linux distribution,  such  as Debian, CentOS, Ubuntu, Fedora, OpenSUSE etc.
The basic steps will be identical and only differ when you get to the installation phase for building your new VM;
which is no different than building any fresh installation on an actual physical machine.
We will give step-by-step instructions with screen capture images;
If you feel confident, please try to just launch virt-manager and see if you can work your way through the necessary steps, as the GUI is reasonably clearly constructed.
1.
Make sure libvirtd is running and start virt-manager by typing: `$ sudo systemctl start libvirtd` `$ sudo virt-manager`
